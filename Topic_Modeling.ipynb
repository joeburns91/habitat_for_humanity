{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b9638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joebu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "c:\\users\\joebu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\users\\joebu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "\n",
    "#Base and Cleaning \n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import regex\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis.gensim\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py \n",
    "import chart_studio.tools as tls\n",
    "\n",
    "#Natural Language Processing (NLP)\n",
    "import spacy\n",
    "import gensim\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd548fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Message</th>\n",
       "      <th>emoji_free_tweets</th>\n",
       "      <th>url_free_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Post Message  \\\n",
       "0                                                nan   \n",
       "1  Join our team as a full-time ReStore Truck Dri...   \n",
       "2  Research has shown that when single women emba...   \n",
       "3  Today, on International Women's Day, we celebr...   \n",
       "4  Clouds come floating into my life, no longer t...   \n",
       "\n",
       "                                   emoji_free_tweets  \\\n",
       "0                                                nan   \n",
       "1  Join our team as a full-time ReStore Truck Dri...   \n",
       "2  Research has shown that when single women emba...   \n",
       "3  Today, on International Women's Day, we celebr...   \n",
       "4  Clouds come floating into my life, no longer t...   \n",
       "\n",
       "                                     url_free_tweets  \n",
       "0                                                nan  \n",
       "1  Join our team as a full-time ReStore Truck Dri...  \n",
       "2  Research has shown that when single women emba...  \n",
       "3  Today, on International Women's Day, we celebr...  \n",
       "4  Clouds come floating into my life, no longer t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Converting the dataset to pandas DataFrame and renaming the columns \n",
    "df = pd.read_csv('Habitat_Post_level_Merged.csv')\n",
    "df['Post Message'] = df['Post Message'].astype(str)\n",
    "\n",
    "#Removing emojies from text\n",
    "#Refrence 1 : https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "#Refrence 2 : https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def url_free_text(text):\n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the function above and get tweets free of emoji's\n",
    "call_emoji_free = lambda x: give_emoji_free_text(x)\n",
    "\n",
    "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
    "df['emoji_free_tweets'] = df['Post Message'].apply(call_emoji_free)\n",
    "\n",
    "#Create a new column with url free tweets\n",
    "df['url_free_tweets'] = df['emoji_free_tweets'].apply(url_free_text)\n",
    "\n",
    "df = df[[\"Post Message\" , \"emoji_free_tweets\", \"url_free_tweets\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56cc8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "# Make sure to restart the runtime after running installations and libraries tab\n",
    "# nlp = spacy.load(\"en_core_web_trf\")\n",
    "import en_core_web_trf\n",
    "nlp = en_core_web_trf.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf6224d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Message</th>\n",
       "      <th>emoji_free_tweets</th>\n",
       "      <th>url_free_tweets</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>[join, team, full-time, restore, truck, driver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>[research, shown, single, women, embark, conve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>[today,, international, women's, day,, celebra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>[clouds, come, floating, life,, longer, carry,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>[sense, connectedness, willingness, actively, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>[covid-19, forced, deeply, consider, fundament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>[deserves, decent, affordable, place, live.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>[build, strength,, stability,, self-reliance, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>[companies, encourage, employees, volunteer, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Post Message  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                     emoji_free_tweets  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                       url_free_tweets  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                                tokens  \n",
       "0                                                [nan]  \n",
       "1    [join, team, full-time, restore, truck, driver...  \n",
       "2    [research, shown, single, women, embark, conve...  \n",
       "3    [today,, international, women's, day,, celebra...  \n",
       "4    [clouds, come, floating, life,, longer, carry,...  \n",
       "..                                                 ...  \n",
       "408  [sense, connectedness, willingness, actively, ...  \n",
       "409  [covid-19, forced, deeply, consider, fundament...  \n",
       "410       [deserves, decent, affordable, place, live.]  \n",
       "411  [build, strength,, stability,, self-reliance, ...  \n",
       "412  [companies, encourage, employees, volunteer, t...  \n",
       "\n",
       "[413 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import Gensim and Wordcloud to use their stopwords as well and use the combined stopwords of ALL as the variable:\n",
    "ALL_STOP_WORDS\n",
    "\"\"\"\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
    "\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df['url_free_tweets'], batch_size=500):\n",
    "    doc_tokens = []    \n",
    "    for token in doc: \n",
    "        if token.text.lower() not in STOP_WORDS:\n",
    "            doc_tokens.append(token.text.lower())   \n",
    "    tokens.append(doc_tokens)\n",
    "\n",
    "# Makes tokens column\n",
    "df['tokens'] = tokens\n",
    "\n",
    "# View df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a95cb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joebu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Message</th>\n",
       "      <th>emoji_free_tweets</th>\n",
       "      <th>url_free_tweets</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_back_to_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>[join, team, full-time, restore, truck, driver...</td>\n",
       "      <td>join team full-time restore truck driver! pers...</td>\n",
       "      <td>[join, team, time, restore, truck, driver, per...</td>\n",
       "      <td>join team time restore truck driver person ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>[research, shown, single, women, embark, conve...</td>\n",
       "      <td>research shown single women embark conventiona...</td>\n",
       "      <td>[research, show, single, woman, embark, conven...</td>\n",
       "      <td>research show single woman embark conventional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>[today,, international, women's, day,, celebra...</td>\n",
       "      <td>today, international women's day, celebrate st...</td>\n",
       "      <td>[today, international, woman, day, celebrate, ...</td>\n",
       "      <td>today international woman day celebrate strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>[clouds, come, floating, life,, longer, carry,...</td>\n",
       "      <td>clouds come floating life, longer carry rain u...</td>\n",
       "      <td>[cloud, come, float, life, long, carry, rain, ...</td>\n",
       "      <td>cloud come float life long carry rain usher st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>[sense, connectedness, willingness, actively, ...</td>\n",
       "      <td>sense connectedness willingness actively help ...</td>\n",
       "      <td>[sense, connectedness, willingness, actively, ...</td>\n",
       "      <td>sense connectedness willingness actively help ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>[covid-19, forced, deeply, consider, fundament...</td>\n",
       "      <td>covid-19 forced deeply consider fundamental im...</td>\n",
       "      <td>[covid-19, force, deeply, consider, fundamenta...</td>\n",
       "      <td>covid-19 force deeply consider fundamental imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>[deserves, decent, affordable, place, live.]</td>\n",
       "      <td>deserves decent affordable place live.</td>\n",
       "      <td>[deserve, decent, affordable, place, live]</td>\n",
       "      <td>deserve decent affordable place live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>[build, strength,, stability,, self-reliance, ...</td>\n",
       "      <td>build strength, stability, self-reliance shelter.</td>\n",
       "      <td>[build, strength, stability, self, reliance, s...</td>\n",
       "      <td>build strength stability self reliance shelter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>[companies, encourage, employees, volunteer, t...</td>\n",
       "      <td>companies encourage employees volunteer time n...</td>\n",
       "      <td>[company, encourage, employee, volunteer, time...</td>\n",
       "      <td>company encourage employee volunteer time nonp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Post Message  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                     emoji_free_tweets  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                       url_free_tweets  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0                                                [nan]   \n",
       "1    [join, team, full-time, restore, truck, driver...   \n",
       "2    [research, shown, single, women, embark, conve...   \n",
       "3    [today,, international, women's, day,, celebra...   \n",
       "4    [clouds, come, floating, life,, longer, carry,...   \n",
       "..                                                 ...   \n",
       "408  [sense, connectedness, willingness, actively, ...   \n",
       "409  [covid-19, forced, deeply, consider, fundament...   \n",
       "410       [deserves, decent, affordable, place, live.]   \n",
       "411  [build, strength,, stability,, self-reliance, ...   \n",
       "412  [companies, encourage, employees, volunteer, t...   \n",
       "\n",
       "                                   tokens_back_to_text  \\\n",
       "0                                                  nan   \n",
       "1    join team full-time restore truck driver! pers...   \n",
       "2    research shown single women embark conventiona...   \n",
       "3    today, international women's day, celebrate st...   \n",
       "4    clouds come floating life, longer carry rain u...   \n",
       "..                                                 ...   \n",
       "408  sense connectedness willingness actively help ...   \n",
       "409  covid-19 forced deeply consider fundamental im...   \n",
       "410             deserves decent affordable place live.   \n",
       "411  build strength, stability, self-reliance shelter.   \n",
       "412  companies encourage employees volunteer time n...   \n",
       "\n",
       "                                                lemmas  \\\n",
       "0                                                [nan]   \n",
       "1    [join, team, time, restore, truck, driver, per...   \n",
       "2    [research, show, single, woman, embark, conven...   \n",
       "3    [today, international, woman, day, celebrate, ...   \n",
       "4    [cloud, come, float, life, long, carry, rain, ...   \n",
       "..                                                 ...   \n",
       "408  [sense, connectedness, willingness, actively, ...   \n",
       "409  [covid-19, force, deeply, consider, fundamenta...   \n",
       "410         [deserve, decent, affordable, place, live]   \n",
       "411  [build, strength, stability, self, reliance, s...   \n",
       "412  [company, encourage, employee, volunteer, time...   \n",
       "\n",
       "                                   lemmas_back_to_text  \n",
       "0                                                  nan  \n",
       "1    join team time restore truck driver person ser...  \n",
       "2    research show single woman embark conventional...  \n",
       "3    today international woman day celebrate strong...  \n",
       "4    cloud come float life long carry rain usher st...  \n",
       "..                                                 ...  \n",
       "408  sense connectedness willingness actively help ...  \n",
       "409  covid-19 force deeply consider fundamental imp...  \n",
       "410               deserve decent affordable place live  \n",
       "411     build strength stability self reliance shelter  \n",
       "412  company encourage employee volunteer time nonp...  \n",
       "\n",
       "[413 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Refrence 4 : https://stackoverflow.com/questions/45306988/column-of-lists-convert-list-to-string-as-a-new-column\n",
    "\n",
    "# Make tokens a string again\n",
    "df['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''Used to lemmatize the processed tweets'''\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Something goes here :P\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "df['lemmas'] = df['tokens_back_to_text'].apply(get_lemmas)\n",
    "\n",
    "# Make lemmas a string again\n",
    "df['lemmas_back_to_text'] = [' '.join(map(str, l)) for l in df['lemmas']]\n",
    "# df[['original_tweet', 'lemmas_back_to_text']]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf5d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:20: DeprecationWarning: invalid escape sequence \\$\n",
      "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:20: DeprecationWarning: invalid escape sequence \\$\n",
      "<>:19: DeprecationWarning: invalid escape sequence \\w\n",
      "<>:20: DeprecationWarning: invalid escape sequence \\$\n",
      "C:\\Users\\joebu\\AppData\\Local\\Temp\\ipykernel_29524\\3047486865.py:19: DeprecationWarning: invalid escape sequence \\w\n",
      "  tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
      "C:\\Users\\joebu\\AppData\\Local\\Temp\\ipykernel_29524\\3047486865.py:20: DeprecationWarning: invalid escape sequence \\$\n",
      "  tokens = re.sub('@*!*\\$*', '', text) # Remove @ ! $\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post Message</th>\n",
       "      <th>emoji_free_tweets</th>\n",
       "      <th>url_free_tweets</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_back_to_text</th>\n",
       "      <th>lemma_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>Join our team as a full-time ReStore Truck Dri...</td>\n",
       "      <td>[join, team, full-time, restore, truck, driver...</td>\n",
       "      <td>join team full-time restore truck driver! pers...</td>\n",
       "      <td>[join, team, time, restore, truck, driver, per...</td>\n",
       "      <td>join team time restore truck driver person ser...</td>\n",
       "      <td>[join, team, time, restore, truck, driver, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>Research has shown that when single women emba...</td>\n",
       "      <td>[research, shown, single, women, embark, conve...</td>\n",
       "      <td>research shown single women embark conventiona...</td>\n",
       "      <td>[research, show, single, woman, embark, conven...</td>\n",
       "      <td>research show single woman embark conventional...</td>\n",
       "      <td>[research, show, single, woman, embark, conven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>Today, on International Women's Day, we celebr...</td>\n",
       "      <td>[today,, international, women's, day,, celebra...</td>\n",
       "      <td>today, international women's day, celebrate st...</td>\n",
       "      <td>[today, international, woman, day, celebrate, ...</td>\n",
       "      <td>today international woman day celebrate strong...</td>\n",
       "      <td>[today, international, woman, day, celebrate, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>Clouds come floating into my life, no longer t...</td>\n",
       "      <td>[clouds, come, floating, life,, longer, carry,...</td>\n",
       "      <td>clouds come floating life, longer carry rain u...</td>\n",
       "      <td>[cloud, come, float, life, long, carry, rain, ...</td>\n",
       "      <td>cloud come float life long carry rain usher st...</td>\n",
       "      <td>[cloud, come, float, life, long, carry, rain, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>A sense of connectedness and a willingness to ...</td>\n",
       "      <td>[sense, connectedness, willingness, actively, ...</td>\n",
       "      <td>sense connectedness willingness actively help ...</td>\n",
       "      <td>[sense, connectedness, willingness, actively, ...</td>\n",
       "      <td>sense connectedness willingness actively help ...</td>\n",
       "      <td>[sense, connectedness, willingness, actively, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>COVID-19 has forced everyone to deeply conside...</td>\n",
       "      <td>[covid-19, forced, deeply, consider, fundament...</td>\n",
       "      <td>covid-19 forced deeply consider fundamental im...</td>\n",
       "      <td>[covid-19, force, deeply, consider, fundamenta...</td>\n",
       "      <td>covid-19 force deeply consider fundamental imp...</td>\n",
       "      <td>[covid-19, force, deeply, consider, fundamenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>Everyone deserves a decent and affordable plac...</td>\n",
       "      <td>[deserves, decent, affordable, place, live.]</td>\n",
       "      <td>deserves decent affordable place live.</td>\n",
       "      <td>[deserve, decent, affordable, place, live]</td>\n",
       "      <td>deserve decent affordable place live</td>\n",
       "      <td>[deserve, decent, affordable, place, live]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>We build strength, stability, and self-relianc...</td>\n",
       "      <td>[build, strength,, stability,, self-reliance, ...</td>\n",
       "      <td>build strength, stability, self-reliance shelter.</td>\n",
       "      <td>[build, strength, stability, self, reliance, s...</td>\n",
       "      <td>build strength stability self reliance shelter</td>\n",
       "      <td>[build, strength, stability, self, reliance, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>Many companies not only encourage their employ...</td>\n",
       "      <td>[companies, encourage, employees, volunteer, t...</td>\n",
       "      <td>companies encourage employees volunteer time n...</td>\n",
       "      <td>[company, encourage, employee, volunteer, time...</td>\n",
       "      <td>company encourage employee volunteer time nonp...</td>\n",
       "      <td>[company, encourage, employee, volunteer, time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>413 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Post Message  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                     emoji_free_tweets  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                       url_free_tweets  \\\n",
       "0                                                  nan   \n",
       "1    Join our team as a full-time ReStore Truck Dri...   \n",
       "2    Research has shown that when single women emba...   \n",
       "3    Today, on International Women's Day, we celebr...   \n",
       "4    Clouds come floating into my life, no longer t...   \n",
       "..                                                 ...   \n",
       "408  A sense of connectedness and a willingness to ...   \n",
       "409  COVID-19 has forced everyone to deeply conside...   \n",
       "410  Everyone deserves a decent and affordable plac...   \n",
       "411  We build strength, stability, and self-relianc...   \n",
       "412  Many companies not only encourage their employ...   \n",
       "\n",
       "                                                tokens  \\\n",
       "0                                                [nan]   \n",
       "1    [join, team, full-time, restore, truck, driver...   \n",
       "2    [research, shown, single, women, embark, conve...   \n",
       "3    [today,, international, women's, day,, celebra...   \n",
       "4    [clouds, come, floating, life,, longer, carry,...   \n",
       "..                                                 ...   \n",
       "408  [sense, connectedness, willingness, actively, ...   \n",
       "409  [covid-19, forced, deeply, consider, fundament...   \n",
       "410       [deserves, decent, affordable, place, live.]   \n",
       "411  [build, strength,, stability,, self-reliance, ...   \n",
       "412  [companies, encourage, employees, volunteer, t...   \n",
       "\n",
       "                                   tokens_back_to_text  \\\n",
       "0                                                  nan   \n",
       "1    join team full-time restore truck driver! pers...   \n",
       "2    research shown single women embark conventiona...   \n",
       "3    today, international women's day, celebrate st...   \n",
       "4    clouds come floating life, longer carry rain u...   \n",
       "..                                                 ...   \n",
       "408  sense connectedness willingness actively help ...   \n",
       "409  covid-19 forced deeply consider fundamental im...   \n",
       "410             deserves decent affordable place live.   \n",
       "411  build strength, stability, self-reliance shelter.   \n",
       "412  companies encourage employees volunteer time n...   \n",
       "\n",
       "                                                lemmas  \\\n",
       "0                                                [nan]   \n",
       "1    [join, team, time, restore, truck, driver, per...   \n",
       "2    [research, show, single, woman, embark, conven...   \n",
       "3    [today, international, woman, day, celebrate, ...   \n",
       "4    [cloud, come, float, life, long, carry, rain, ...   \n",
       "..                                                 ...   \n",
       "408  [sense, connectedness, willingness, actively, ...   \n",
       "409  [covid-19, force, deeply, consider, fundamenta...   \n",
       "410         [deserve, decent, affordable, place, live]   \n",
       "411  [build, strength, stability, self, reliance, s...   \n",
       "412  [company, encourage, employee, volunteer, time...   \n",
       "\n",
       "                                   lemmas_back_to_text  \\\n",
       "0                                                  nan   \n",
       "1    join team time restore truck driver person ser...   \n",
       "2    research show single woman embark conventional...   \n",
       "3    today international woman day celebrate strong...   \n",
       "4    cloud come float life long carry rain usher st...   \n",
       "..                                                 ...   \n",
       "408  sense connectedness willingness actively help ...   \n",
       "409  covid-19 force deeply consider fundamental imp...   \n",
       "410               deserve decent affordable place live   \n",
       "411     build strength stability self reliance shelter   \n",
       "412  company encourage employee volunteer time nonp...   \n",
       "\n",
       "                                          lemma_tokens  \n",
       "0                                                [nan]  \n",
       "1    [join, team, time, restore, truck, driver, per...  \n",
       "2    [research, show, single, woman, embark, conven...  \n",
       "3    [today, international, woman, day, celebrate, ...  \n",
       "4    [cloud, come, float, life, long, carry, rain, ...  \n",
       "..                                                 ...  \n",
       "408  [sense, connectedness, willingness, actively, ...  \n",
       "409  [covid-19, force, deeply, consider, fundamenta...  \n",
       "410         [deserve, decent, affordable, place, live]  \n",
       "411  [build, strength, stability, self, reliance, s...  \n",
       "412  [company, encourage, employee, volunteer, time...  \n",
       "\n",
       "[413 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Parses a string into a list of semantic units (words)\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "    Returns:\n",
    "        list: tokens parsed out\n",
    "    \"\"\"\n",
    "    # Removing url's\n",
    "    pattern = r\"http\\S+\"\n",
    "    \n",
    "    tokens = re.sub(pattern, \"\", text) # https://www.youtube.com/watch?v=O2onA4r5UaY\n",
    "    tokens = re.sub('[^a-zA-Z 0-9]', '', text)\n",
    "    tokens = re.sub('[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
    "    tokens = re.sub('\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
    "    tokens = re.sub('@*!*\\$*', '', text) # Remove @ ! $\n",
    "    tokens = tokens.strip(',') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('?') # TESTING THIS LINE\n",
    "    tokens = tokens.strip('!') # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\"'\") # TESTING THIS LINE\n",
    "    tokens = tokens.strip(\".\") # TESTING THIS LINE\n",
    "\n",
    "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenizer\n",
    "df['lemma_tokens'] = df['lemmas_back_to_text'].apply(tokenize)\n",
    "\n",
    "# View those tokens (the 4th column)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c302a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2449\n"
     ]
    }
   ],
   "source": [
    "# Create a id2word dictionary\n",
    "id2word = Dictionary(df['lemma_tokens'])\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e743fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138\n"
     ]
    }
   ],
   "source": [
    "# Filtering Extremes\n",
    "id2word.filter_extremes(no_below=2, no_above=.99)\n",
    "print(len(id2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2035ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a corpus object \n",
    "corpus = [id2word.doc2bow(d) for d in df['lemma_tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4a2d4",
   "metadata": {},
   "source": [
    "# 4 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be41c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=4, id2word=id2word, workers=12, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbaec2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d398b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "485898d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "home matthews volunteer family thank health time great community repair\n",
      "\n",
      "------ Topic 1 ------\n",
      "home build community family matthews habitat help repair program year\n",
      "\n",
      "------ Topic 2 ------\n",
      "housing habitat affordable home humanity family help work matthews build\n",
      "\n",
      "------ Topic 3 ------\n",
      "home community habitat build gift housing live match good americans\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b553d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.695508571480972\n",
      "\n",
      "Coherence Score:  0.3729640989598583\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217c4e6",
   "metadata": {},
   "source": [
    "# 3 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa437d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=3, id2word=id2word, workers=12, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a802ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26a5b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb257d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "build habitat home family housing help donate humanity learn affordable\n",
      "\n",
      "------ Topic 1 ------\n",
      "housing community home affordable matthews habitat income need build household\n",
      "\n",
      "------ Topic 2 ------\n",
      "home habitat family matthews help humanity repair place build live\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eae074ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.62849647317972\n",
      "\n",
      "Coherence Score:  0.3744647840670063\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145a37c",
   "metadata": {},
   "source": [
    "# 6 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ec0be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=6, id2word=id2word, workers=12, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5aefd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b506f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d705aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "home community family housing affordable repair health homeownership habitat improve\n",
      "\n",
      "------ Topic 1 ------\n",
      "housing home matthews application family community affordable hour place nan\n",
      "\n",
      "------ Topic 2 ------\n",
      "home housing year program repair gift visit increase match need\n",
      "\n",
      "------ Topic 3 ------\n",
      "habitat matthews housing help build family home humanity community learn\n",
      "\n",
      "------ Topic 4 ------\n",
      "volunteer build work community construction thank home matthews church team\n",
      "\n",
      "------ Topic 5 ------\n",
      "home habitat family humanity world place matthews build happy live\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8f54bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.7918476618903085\n",
      "\n",
      "Coherence Score:  0.4585735703598634\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb75f18",
   "metadata": {},
   "source": [
    "# 9 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d13990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=9, id2word=id2word, workers=12, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88d05fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c86782b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "856f0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "habitat build black community donate humanity housing family home dream\n",
      "\n",
      "------ Topic 1 ------\n",
      "home community habitat build housing old matthews gift time opportunity\n",
      "\n",
      "------ Topic 2 ------\n",
      "housing application affordable matthews habitat community humanity homeownership income year\n",
      "\n",
      "------ Topic 3 ------\n",
      "housing affordable access nan increase year family house day economic\n",
      "\n",
      "------ Topic 4 ------\n",
      "home habitat family humanity build world place decent work live\n",
      "\n",
      "------ Topic 5 ------\n",
      "housing matthews habitat family affordable humanity greater restore build home\n",
      "\n",
      "------ Topic 6 ------\n",
      "home help family old place americans need matthews habitat program\n",
      "\n",
      "------ Topic 7 ------\n",
      "home volunteer family build matthews church 💚 thank dollar housing\n",
      "\n",
      "------ Topic 8 ------\n",
      "home repair program help critical day learn good family community\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f317f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.861622561935722\n",
      "\n",
      "Coherence Score:  0.38750917719363787\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0736c",
   "metadata": {},
   "source": [
    "# 7 Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcdb3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating a LDA model \n",
    "base_model = LdaMulticore(corpus=corpus, num_topics=7, id2word=id2word, workers=12, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76824d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for words \n",
    "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a68a1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Topics\n",
    "topics = [' '.join(t[0:10]) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e259add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Topic 0 ------\n",
      "home build community affordable family housing habitat decent help need\n",
      "\n",
      "------ Topic 1 ------\n",
      "housing family household home homeownership cost pandemic year income black\n",
      "\n",
      "------ Topic 2 ------\n",
      "home matthews family habitat application repair housing cost volunteer affordable\n",
      "\n",
      "------ Topic 3 ------\n",
      "home family program help habitat matthews build life repair nan\n",
      "\n",
      "------ Topic 4 ------\n",
      "habitat matthews humanity home community old learn greater housing work\n",
      "\n",
      "------ Topic 5 ------\n",
      "habitat build humanity volunteer world day home help happy know\n",
      "\n",
      "------ Topic 6 ------\n",
      "housing home church matthews thank habitat americans community help new\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the topics\n",
    "for id, t in enumerate(topics): \n",
    "    print(f\"------ Topic {id} ------\")\n",
    "    print(t, end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89a147a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.767383419788621\n",
      "\n",
      "Coherence Score:  0.41252684039245147\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better\n",
    "base_perplexity = base_model.log_perplexity(corpus)\n",
    "print('\\nPerplexity: ', base_perplexity) \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=base_model, texts=df['lemma_tokens'], \n",
    "                                   dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_model_base = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_model_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d6772a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
